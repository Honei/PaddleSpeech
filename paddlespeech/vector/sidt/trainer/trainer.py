#!/usr/bin/env python
# -*- coding: utf-8 -*-
########################################################################
#
# Copyright     2020    Zeng Xingui(zengxingui@baidu.com)
#
########################################################################

"""
Load nnet3 training egs which generated by kaldi
"""
from __future__ import print_function
import os
import sys
import subprocess
import copy
from argparse import ArgumentParser

import paddle
from paddle import Model
from paddle.io import DataLoader, Dataset, DistributedBatchSampler
from paddle.hapi.callbacks import config_callbacks
from paddle.nn import Layer
import warnings

from sidt import _logger as log
from sidt.callbacks.callbacks import update_callbacks
from sidt.utils.seed import seed_everything

warnings.filterwarnings("ignore", category=DeprecationWarning)

BN_TYPES = (paddle.nn.BatchNorm1D, paddle.nn.BatchNorm2D, paddle.nn.BatchNorm3D)

class Trainer(Model):
    """
    An Model object is network with training and inference features.
    Only dynamic graph is supported. The usage is as follows.

    Args:
        network (paddle.nn.Layer): The network is an instance of
            paddle.nn.Layer.

    Examples:
        .. code-block:: python

        import paddle
        import paddle.nn as nn
        import sidt

        device = paddle.set_device('cpu') # or 'gpu'
        paddle.disable_static(device)

        net = nn.Sequential(
            nn.Linear(784, 200),
            nn.Tanh(),
            nn.Linear(200, 10))

        model = sidt.Trainer(net)
        optim = paddle.optimizer.SGD(learning_rate=1e-3,
            parameters=model.parameters())
        model.prepare(optim,
                      paddle.nn.CrossEntropyLoss(),
                      paddle.metric.Accuracy())

        data = paddle.vision.datasets.MNIST(mode='train', chw_format=False)
        model.fit(data, epochs=2, batch_size=32, verbose=1)
    """

    def __init__(self, *args, **kwargs):
        super(Trainer, self).__init__(*args, **kwargs)
        self.has_load_pretrained_model = False

    def prepare(self, optimizer=None, loss=None, metrics=None):
        """
        Configures the model before runing.

        Args:
            optimizer (Optimizer|None): Optimizer must be set in training
                and should be a Optimizer instance. It can be None in eval
                and test mode.
            loss (Loss|callable function|None): Loss function can
                be a `paddle.nn.Layer` instance or any callable function
                taken the predicted values and ground truth values as input.
                It can be None when there is no loss.
            metrics (Metric|list of Metric|None): If metrics is set, all
                metrics will be calculated and output in train/eval mode.

        Returns:
            None
        """
        super(Trainer, self).prepare(optimizer, loss, metrics)

    def fit(self,
            train_data=None,
            eval_data=None,
            batch_size=1,
            eval_batch_size=1,
            bg_epoch=0,
            epochs=1,
            eval_freq=1,
            log_freq=10,
            pretrained_model=None,
            save_dir=None,
            save_freq=1,
            verbose=2,
            drop_last=False,
            shuffle=False,
            num_workers=False,
            callbacks=None,
            seed=0,
            accumulate_grad_batches=1,
            *args,
            **kwargs
           ):
        """
        Trains the model for a fixed number of epochs. If `eval_data` is set,
        evaluation will be done at the end of each epoch.

        Args:
            train_data (Dataset): An iterable data loader is used for 
                train. An instance of paddle paddle.io.Dataset is recomended. Default: None.
            eval_data (Dataset): An iterable data loader is used for
                evaluation at the end of epoch. If None, will not do evaluation. 
                An instance of paddle.io.Dataset is recomended. Default: None.
            batch_size (int): Integer number. The batch size of train_data.
                When train_data are the instance of Dataloader,
                this parameter will be ignored. Default: 1.
            eval_batch_size (int): Integer number. The batch size of eval_data.
                When eval_data are the instance of Dataloader,
                this parameter will be ignored. Default: 1.
            bg_epoch (int): Integer number. The index of first epoch to train
                the model. Default: 0.
            epochs (int): Integer number. The number of epochs to train
                the model. Default: 1.
            eval_freq (int): The frequency, in number of epochs, an evalutation
                is performed. Default: 1.
            log_freq (int): The frequency, in number of steps, the training logs
                are printed. Default: 10.
            pretrained_model (string|None): The directory of pretrained model.
                If None, will train from scrach. Default: None
            save_dir(str|None): The directory to save checkpoint during training.
                If None, will not save checkpoint. Default: None.
            save_freq (int): The frequency, in number of epochs, to save
                checkpoint. Default: 1.
            verbose (int): The verbosity mode, should be 0, 1, or 2. 0 = silent,
                1 = progress bar, 2 = one line per epoch. Default: 2.
            drop_last (bool): Whether drop the last incomplete batch of
                train_data when dataset size is not divisible by the batch size.
                When train_data is an instance of Dataloader, this parameter
                will be ignored. Default: False.
            shuffle (bool): Whther to shuffle train_data. When train_data is
                an instance of Dataloader, this parameter will be ignored.
                Default: True.
            num_workers (int): The number of subprocess to load data, 0 for no
                subprocess used and loading data in main process.
                When train_data and eval_data are both the instance of
                Dataloader, this parameter will be ignored. Default: 0.
            callbacks (Callback|None): A list of `Callback` instances to apply
                during training. If None, `ProgBarLogger` and `ModelCheckpoint`
                are automatically inserted. Default: None.
            seed (int): seed for all random function, Default: 0

        Returns:
            None

        Examples:
            1. An example use Dataset and set btch size, shuffle in fit.
               How to make a batch is done internally.

            .. code-block:: python

              import paddle
              import sidt

              dynamic = True
              device = paddle.set_device('cpu') # or 'gpu'
              paddle.disable_static(device) if dynamic else None

              train_dataset = paddle.vision.datasets.MNIST(mode='train')
              val_dataset = paddle.vision.datasets.MNIST(mode='test')

              model = sidt.Trainer(
                  paddle.vision.models.LeNet(classifier_activation=None))
              optim = paddle.optimizer.Adam(
                  learning_rate=0.001, parameters=model.parameters())
              model.prepare(
                  optim,
                  paddle.nn.CrossEntropyLoss(),
                  paddle.metric.Accuracy(topk=(1, 2)))
              model.fit(train_dataset,
                        val_dataset,
                        epochs=2,
                        batch_size=64,
                        save_dir='mnist_checkpoint')
        """

        assert train_data is not None, \
                "train_data must be given!"

        if bg_epoch >= epochs:
            log.info("The begin epoch({1}) is not less than end epoch({2})".format(bg_epoch, epochs))
            return

        if not self.has_load_pretrained_model and pretrained_model is not None:
            self.load_pretrained_model(pretrained_model)

        if eval_data is not None and isinstance(eval_data, Dataset):
            eval_batch_size = eval_batch_size if not hasattr(eval_data, 'next_subset') else 1
            eval_sampler = DistributedBatchSampler(
                eval_data, batch_size=eval_batch_size)
            eval_collate_fn = eval_data.collate_fn if hasattr(eval_data, 'collate_fn') else None
            eval_loader = DataLoader(
                eval_data,
                batch_sampler=eval_sampler,
                places=self._place,
                num_workers=num_workers,
                return_list=True,
                collate_fn=eval_collate_fn)
        elif eval_data is not None:
            eval_loader = eval_data
        else:
            eval_loader = None

        do_eval = eval_loader is not None
        self._test_dataloader = eval_loader
        self._accumulate = accumulate_grad_batches
        cbks = config_callbacks(
            callbacks,
            model=self,
            epochs=epochs,
            steps=0,
            log_freq=log_freq,
            save_freq=save_freq,
            save_dir=save_dir,
            verbose=verbose,
            metrics=self._metrics_name(), )

        cbks.on_begin('train')
        for epoch in range(bg_epoch, epochs):
            seed_everything(seed)
            print("batch num: {}".format(len(train_data)))
            if isinstance(train_data, Dataset):
                train_sampler = DistributedBatchSampler(
                    train_data,
                    batch_size=batch_size,
                    shuffle=shuffle,
                    drop_last=drop_last)
                train_collate_fn = train_data.collate_fn if hasattr(train_data, 'collate_fn') else None
                train_loader = DataLoader(
                    train_data,
                    batch_sampler=train_sampler,
                    places=self._place,
                    num_workers=num_workers,
                    return_list=True,
                    collate_fn=train_collate_fn)
            else:
                train_loader = train_data
            
            steps = self._len_data_loader(train_loader)
            cbks = update_callbacks(cbks, batch_size, epochs,
                                    steps, verbose, self._metrics_name())

            cbks.on_epoch_begin(epoch)
            
            # train one epoch
            logs = self.train_one_epoch(train_loader, cbks, 'train')
            
            # cbks.on_epoch_end(epoch, logs)

            # if do_eval and epoch % eval_freq == 0:

            #     eval_steps = self._len_data_loader(eval_loader)
            #     cbks.on_begin('eval', {
            #         'steps': eval_steps,
            #         'metrics': self._metrics_name()
            #     })

            #     eval_logs = self._run_one_epoch(eval_loader, cbks, 'eval')

            #     cbks.on_end('eval', eval_logs)
            # if hasattr(train_data, 'next_subset'):
            #     train_data.next_subset()

        # cbks.on_end('train', logs)
        self._test_dataloader = None

    def _make_trainable(self, layer):
        """Unfreezes a given Layer.

        Args:
            layer: The layer to unfreeze
        """
        for param in layer.parameters():
            param.trainable = True

        layer.train()

    def _recursive_freeze(self, layer, train_bn):
        """Freezes the sub-layers of a given layer.

        Args:
            layer: The layer to freeze
            train_bn: If True, leave the BatchNorm layers in training mode
        """
        children = list(layer.children())
        if not children:
            if not (isinstance(layer, BN_TYPES) and train_bn):
                for param in layer.parameters():
                    param.trainable = False
                layer.eval()
            else:
                # Make the BN layers trainable
                self._make_trainable(layer)
        else:
            for child in children:
                self._recursive_freeze(layer=child, train_bn=train_bn)

    def freeze(self, n, train_bn=True):
        """Freezes the layers up to index n (if n is not None).

        Args:
            n: Max depth at which we stop freezing the layers. If None, all
                the layers of the given layer will be frozen.
            train_bn: If True, leave the BatchNorm layers in training mode
        """
        children = list(self.network.children())
        n_max = len(children) if n is None else int(n)

        for child in children[:n_max]:
            self._recursive_freeze(layer=child, train_bn=train_bn)

        for child in children[n_max:]:
            self._make_trainable(layer=child)

    def load_pretrained_model(self, pretrained_model):
        """ Load pretrained model.

        Args:
            pretrained_model: pretrained model
        """
        if pretrained_model is not None:
            log.info("Load pretrained model: %s" % (pretrained_model))
            self.load(pretrained_model, skip_mismatch=True)
            self.has_load_pretrained_model = True

    def train_one_epoch(
            self,
            data_loader,
            callbacks,
            mode,
            logs={}, ):
        outputs = []
        print("data loader num: {}".format(len(data_loader)))
        for step, data in enumerate(data_loader):
            # data might come from different types of data_loader and have
            # different format, as following:
            # 1. DataLoader in static graph:
            #    [[input1, input2, ..., label1, lable2, ...]]
            # 2. DataLoader in dygraph
            #    [input1, input2, ..., label1, lable2, ...]
            # 3. custumed iterator yield concated inputs and labels:
            #   [input1, input2, ..., label1, lable2, ...]
            # 4. custumed iterator yield seperated inputs and labels:
            #   ([input1, input2, ...], [label1, lable2, ...])
            # To handle all of these, flatten (nested) list to list.
            data = flatten(data)
            # LoDTensor.shape is callable, where LoDTensor comes from
            # DataLoader in static graph

            batch_size = data[0].shape()[0] if callable(data[
                0].shape) else data[0].shape[0]

            callbacks.on_batch_begin(mode, step, logs)

            if mode != 'predict':

                _inputs = [data[:len(self._inputs)], data[len(self._inputs):]]
                if mode == 'train':
                    _inputs.append((step + 1) % self._accumulate == 0 or
                                   step + 1 == len(data_loader))

                outs = getattr(self, mode + '_batch')(*_inputs)

                if self._metrics and self._loss:
                    metrics = [[l[0] for l in outs[0]]]
                elif self._loss:
                    metrics = [[l[0] for l in outs]]
                else:
                    metrics = []

                # metrics
                for metric in self._metrics:
                    res = metric.accumulate()
                    metrics.extend(to_list(res))

                assert len(self._metrics_name()) == len(metrics)
                for k, v in zip(self._metrics_name(), metrics):
                    logs[k] = v
            else:
                if self._inputs is not None:
                    outs = self.predict_batch(data[:len(self._inputs)])
                else:
                    outs = self.predict_batch(data)

                outputs.append(outs)

            logs['step'] = step
            if mode == 'train' or self._adapter._merge_count.get(
                    mode + '_batch', 0) <= 0:
                logs['batch_size'] = batch_size * ParallelEnv().nranks
            else:
                logs['batch_size'] = self._adapter._merge_count[mode + '_batch']

            callbacks.on_batch_end(mode, step, logs)
            if hasattr(self, 'num_iters') and self.num_iters is not None:
                self.num_iters -= 1
                if self.num_iters <= 0:
                    self.stop_training = True
                    del self.num_iters
                    break
        self._reset_metrics()

        if mode == 'predict':
            return logs, outputs
        return logs

    @staticmethod
    def add_specific_args(parent_parser):
        """
        Add trainer specific args.

        Args:
            parent_parser: an argparse parser.

        Returns:
            parser: an argparse parser.
        """
        parser = ArgumentParser(parents=[parent_parser], add_help=False)
        parser.add_argument("--batch_size", action="store", type=int, default=32,
                help = "Integer number. The batch size of train_data "
                       "and eval_data. When train_data and eval_data are both the "
                       "instance of Dataloader, this parameter will be ignored. Default: 32.")
        parser.add_argument("--bg_epoch", action="store", type=int, default=0,
                            help= "bg_epoch (int): Integer number. The first epoch index. Default: 0")
        parser.add_argument("--epochs", action="store", type=int, default=1, help= \
                            "epochs (int): Integer number. The number of epochs to train the model. Default: 1.")
        parser.add_argument("--eval_freq", action="store", type=int, default=1, help= \
                            "eval_freq (int): The frequency, in number of epochs, "
                            "an evalutation is performed. Default: 1.")
        parser.add_argument("--log_freq", action="store", type=int, default=10, help= \
                            "log_freq (int): The frequency, in number of steps, "
                            "the training logs are printed. Default: 10.")
        parser.add_argument("--save_freq", action="store", type=int, default=1, help= \
                            "save_freq (int): The frequency, in number of epochs, "
                            "to save checkpoint. Default: 1.")
        parser.add_argument("--verbose", action="store", type=int, default=2, help= \
                            "verbose (int): The verbosity mode, should be 0, 1, or 2. "
                            "0 = silent, 1 = progress bar, 2 = one line per epoch. Default: 2.")
        parser.add_argument("--save_dir", action="store", help= \
                            "save_dir(str|None): The directory to save checkpoint during training. "
                            "If None, will not save checkpoint. Default: None.")
        parser.add_argument("--drop_last", action="store_true", default=False, help= \
                            "drop_last (bool): Whether drop the last incomplete batch of "
                            "train_data when dataset size is not divisible by the batch size. "
                            "When train_data is an instance of Dataloader, this parameter "
                            "will be ignored. Default: False.")
        parser.add_argument("--shuffle", action="store_true", default=False, help= \
                            "shuffle (bool): Whther to shuffle train_data. When train_data is "
                            "an instance of Dataloader, this parameter will be ignored. "
                            "Default: True.")
        parser.add_argument("--num_workers", action="store", type=int, default=2, help= \
                            "num_workers (int): The number of subprocess to load data, "
                            "0 for no subprocess used and loading data in main process. "
                            "When train_data and eval_data are both the instance of "
                            "Dataloader, this parameter will be ignored. Default: 0.")
        parser.add_argument("--pretrained_model", action="store", help= \
                            "pretrained_model (string|None): The directory of pretrained model. "
                            "If None, will train from scrach. Default: None")

        return parser


